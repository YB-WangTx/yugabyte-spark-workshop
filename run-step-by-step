--POC cannon
ssh -i "kp-CANNON-aws.pem" centos@50.16.67.143

--Universe: alan-panw-audit-demo
sudo ssh -i /opt/yugabyte/yugaware/data/keys/3315aeea-aa87-43af-9c38-f72c20b9d8c0/yb-demo-aws-poc-selective_3315aeea-aa87-43af-9c38-f72c20b9d8c0-key.pem -ostricthostkeychecking=no -p 22 yugabyte@10.36.1.8

SSL_CERTFILE=~/yugabyte-client-tls-config/ca.crt ./master/bin/ycqlsh  10.36.1.8 9042  -u 'cassandra' --ssl
password: Panwaudit2023!

--Universe: wwang-testing
--login to ycql
./master/bin/ycqlsh `hostname -i`  -u 'cassandra'
password: Yugabyte12#

ssh -i "kp-CANNON-aws.pem" centos@44.214.44.45
cd /home/centos/spark-3.4.1-bin-hadoop3
./bin/spark-shell --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtension --packages com.yugabyte.spark:spark-cassandra-connector_2.12:3.0-yb-8

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import com.datastax.spark.connector._
import org.apache.spark.sql.cassandra.CassandraSQLRow
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector.cql.CassandraConnectorConf
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import com.datastax.spark.connector.cql.CassandraConnector

val host = "10.37.1.250"
val keyspace = "test"
val table = "employees_json_index"
val user = "cassandra"
val password = "Yugabyte12#"

val conf = new SparkConf()
    .setAppName("yb.spark-jsonb")
    .setMaster("local[1]")
    .set("spark.cassandra.connection.localDC", "us-east-1")
    .set("spark.cassandra.connection.host", host)
    .set("spark.sql.catalog.ybcatalog", "com.datastax.spark.connector.datasource.CassandraCatalog")
    .set("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions")

//Spark session
val spark = SparkSession
      .builder()
      .config(conf)
      .config("spark.cassandra.connection.host", host)
      .config("spark.cassandra.connection.port", "9042")
      .config("spark.cassandra.auth.username", user)
      .config("spark.cassandra.auth.password", password)
      .withExtensions(new CassandraSparkExtensions)
      .getOrCreate()

//List namespace
spark.sql("SHOW NAMESPACES FROM ybcatalog").show

//Creating Data Frame by reading testing data from YB cloud database

val df_yb = spark.read.table("ybcatalog.test.employees_json_index")

df_yb.printSchema()
df_yb.count()
df_yb.show()

val df_yb = spark
  .read
  .cassandraFormat("employees_json_index", "test")
  .options(ReadConf.SplitSizeInMBParam.option(32))
  .load()

//Performing ETL : Window function
//row_number()
val windowSpec  = Window.partitionBy("department_id").orderBy("salary")
//ranking: row_number() window function is used to give the sequential row number starting from 1 to the result of each //window partition.
 df_yb.withColumn("row_number",row_number.over(windowSpec)).show()

//rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.
df_yb.withColumn("rank",rank().over(windowSpec)).show()

 //writing back  to YCQL: Persisting a Dataset to Database using Save command, following examples are equivalent
 df.write
   .cassandraFormat("employees_json_copy", "test")
   .mode("overwrite")
   .save()
 
//To verify
val sqlDF = spark.sql("SELECT * FROM ybcatalog.test.employees_json_copy").show(false)

//Native support of Json:
val df = spark.sql("SELECT * FROM ybcatalog.test.employees_json WHERE get_json_object(phone, '$.phone') = 1200");
df.show

//Using JSONB Column Pruning
val query = "SELECT department_id, employee_id, get_json_object(phone, '$.code') as code FROM ybcatalog.test.employees_json WHERE get_json_string(phone, '$.key(1)') = '1400' order by department_id limit 2";
val df_sel1=spark.sql(query)
df_sel1.explain

//Predicate pushed down	   	   
val query = "SELECT department_id, employee_id, get_json_object(phone, '$.key[1].m[2].b') as key FROM ybcatalog.test.employees_json WHERE get_json_string(phone, '$.key[1].m[2].b') = '400' order by department_id limit 2";

val df_sel2 = spark.sql(query)
df_sel2.show()
df_sel2.explain
println("Json processing successful")
  }

--time function
spark.sql("""
SELECT current_timestamp() as ts,
current_timezone() as tz,
current_date() as date,
TIMESTAMP 'yesterday' as yesterday,
TIMESTAMP 'today' as today,
TIMESTAMP 'tomorrow' as tomorrow
""").show(6,0,true)
