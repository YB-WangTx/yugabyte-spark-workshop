cd /home/centos/spark-3.4.1-bin-hadoop3
./bin/spark-shell --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtension --packages com.yugabyte.spark:spark-cassandra-connector_2.12:3.0-yb-8

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import com.datastax.spark.connector._
import org.apache.spark.sql.cassandra.CassandraSQLRow
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector.cql.CassandraConnectorConf
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import com.datastax.spark.connector.cql.CassandraConnector
import java.util.Properties
import org.apache.spark.sql.SaveMode


--process data from YCQL
val host = "10.36.2.198"
val keyspace = "panw_test"
val table = "panw_audit_trail_demo"
val user = "cassandra"
val password = "Panwaudit2023!"
val keyStore ="/home/centos/spark3yb/yb-keystore.jks"

val conf = new SparkConf()
    .setAppName("yb.spark-jsonb")
    .setMaster("local[1]")
    .set("spark.cassandra.connection.localDC", "us-east-1")
    .set("spark.cassandra.connection.host", host)
    .set("spark.sql.catalog.ybcatalog", "com.datastax.spark.connector.datasource.CassandraCatalog")
    .set("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions")

//Spark session to YCQL

 val spark = SparkSession.builder()
             .config(conf)
             .config("spark.cassandra.connection.host", host)
             .config("spark.cassandra.connection.port", "9042")
             .config("spark.cassandra.connection.ssl.clientAuth.enabled", true)
             .config("spark.cassandra.auth.username", user)
             .config("spark.cassandra.auth.password", password)
             .config("spark.cassandra.connection.ssl.enabled", true)
             .config("spark.cassandra.connection.ssl.trustStore.type", "jks")
             .config("spark.cassandra.connection.ssl.trustStore.path", keyStore)
             .config("spark.cassandra.connection.ssl.trustStore.password", "ybcloud")
             .withExtensions(new CassandraSparkExtensions)
             .getOrCreate()

//Spark session
val spark = SparkSession.builder().config(conf).config("spark.cassandra.connection.host", host).config("spark.cassandra.connection.port", "9042").config("spark.cassandra.connection.ssl.clientAuth.enabled", true).config("spark.cassandra.auth.username", user).config("spark.cassandra.auth.password", password).config("spark.cassandra.connection.ssl.enabled", true).config("spark.cassandra.connection.ssl.trustStore.type", "jks").config("spark.cassandra.connection.ssl.trustStore.path", keyStore).config("spark.cassandra.connection.ssl.trustStore.password", "ybcloud").withExtensions(new CassandraSparkExtensions).getOrCreate()

val query = "SELECT tenant_id, get_json_object(data, '$.awsRegion') as awsRgion, get_json_object(data, '$.eventName') as eventName from ybcatalog.panw_test.panw_audit_trail_demo where evt_timestamp between '2023-06-27 12:00:00.0000' and '2023-06-28 12:00:00.00000'";
val df_sel1 = spark.time(spark.sql(query))
spark.time(df_sel1.show())

//group by
df_sel1.groupBy("tenant_id","awsRgion","eventName").count().show(false)

//proepare to write out to YSQL

:require /home/centos/jdbc-yugabytedb-42.3.5-yb-3.jar

val jdbcUrl = s"jdbc:yugabytedb://10.36.2.198:5433/yugabyte"

val connectionProperties = new Properties()
    connectionProperties.put("user", s"yugabyte")
    connectionProperties.put("password", s"Panwaudit2023!")
    connectionProperties.setProperty("Driver", "com.yugabyte.Driver")

//convert DF:df_sel1 to temp table: test1
val test_Df = df_sel1.createOrReplaceTempView("test1")
//write out
spark.table("test1").write.mode(SaveMode.Append).jdbc(jdbcUrl, "ww1", connectionProperties)

//verify
val test_Df = spark.read.jdbc(jdbcUrl, "ww1", connectionProperties)
test_Df.show()
test_Df.printSchema()



