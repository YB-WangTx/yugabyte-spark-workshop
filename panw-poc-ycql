--POC cannon
ssh -i "kp-CANNON-aws.pem" centos@50.16.67.143

--Universe: alan-panw-audit-demo
sudo ssh -i /opt/yugabyte/yugaware/data/keys/3315aeea-aa87-43af-9c38-f72c20b9d8c0/yb-demo-aws-poc-selective_3315aeea-aa87-43af-9c38-f72c20b9d8c0-key.pem -ostricthostkeychecking=no -p 22 yugabyte@10.36.1.8

SSL_CERTFILE=~/yugabyte-client-tls-config/ca.crt ./master/bin/ycqlsh  10.36.1.8 9042  -u 'cassandra' --ssl
password: Panwaudit2023!

keyspace: panw_test
table: panw_audit_trail_demo



--app server
ssh -i "kp-CANNON-aws.pem" centos@44.214.44.45

--Create a keystore to access ssl enabled YCQL
scp -i "kp-CANNON-aws.pem" ~/Downloads/alan.crt centos@44.214.44.45:spark3yb/root.crt

keytool -keystore yb-keystore.jks -storetype 'jks' -importcert -file ~/spark3yb/root.crt -keypass 'ybcloud' 
       -storepass 'ybcloud' -alias ~/home/centos/spark3yb/root_crt  -noprompt

--verify
keytool -list -keystore yb-keystore.jks -storepass ybcloud

--start Spark
cd /home/centos/spark-3.4.1-bin-hadoop3
./bin/spark-shell --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtension --packages com.yugabyte.spark:spark-cassandra-connector_2.12:3.0-yb-8

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import com.datastax.spark.connector._
import org.apache.spark.sql.cassandra.CassandraSQLRow
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector.cql.CassandraConnectorConf
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import com.datastax.spark.connector.cql.CassandraConnector
import java.util.Properties

val host = "10.36.2.198"
val keyspace = "panw_test"
val table = "panw_audit_trail_demo"
val user = "cassandra"
val password = "Panwaudit2023!"
val keyStore ="/home/centos/spark3yb/yb-keystore.jks"

val conf = new SparkConf()
    .setAppName("yb.spark-jsonb")
    .setMaster("local[1]")
    .set("spark.cassandra.connection.localDC", "us-east-1")
    .set("spark.cassandra.connection.host", host)
    .set("spark.sql.catalog.ybcatalog", "com.datastax.spark.connector.datasource.CassandraCatalog")
    .set("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions")

//Spark session to YCQL

 val spark = SparkSession.builder()
             .config(conf)
             .config("spark.cassandra.connection.host", host)
             .config("spark.cassandra.connection.port", "9042")
             .config("spark.cassandra.connection.ssl.clientAuth.enabled", true)
             .config("spark.cassandra.auth.username", user)
             .config("spark.cassandra.auth.password", password)
             .config("spark.cassandra.connection.ssl.enabled", true)
             .config("spark.cassandra.connection.ssl.trustStore.type", "jks")
             .config("spark.cassandra.connection.ssl.trustStore.path", keyStore)
             .config("spark.cassandra.connection.ssl.trustStore.password", "ybcloud")
             .withExtensions(new CassandraSparkExtensions)
             .getOrCreate()

//Spark session
val spark = SparkSession.builder().config(conf).config("spark.cassandra.connection.host", host).config("spark.cassandra.connection.port", "9042").config("spark.cassandra.connection.ssl.clientAuth.enabled", true).config("spark.cassandra.auth.username", user).config("spark.cassandra.auth.password", password).config("spark.cassandra.connection.ssl.enabled", true).config("spark.cassandra.connection.ssl.trustStore.type", "jks").config("spark.cassandra.connection.ssl.trustStore.path", keyStore).config("spark.cassandra.connection.ssl.trustStore.password", "ybcloud").withExtensions(new CassandraSparkExtensions).getOrCreate()


//List namespace
spark.sql("SHOW NAMESPACES FROM ybcatalog").show

//Creating Data Frame by reading testing data from YB cloud database

val df_yb = spark.read.table("ybcatalog.panw_test.panw_audit_trail_demo")

df_yb.printSchema()
df_yb.count()
df_yb.show()

//val df_yb = spark.read.cassandraFormat("panw_audit_trail_demo", "panw_test").options(ReadConf.SplitSizeInMBParam.option(32)).load()

 val df_yb = spark.read.table("ybcatalog.panw_test.panw_audit_trail_demo")
// Perform SQL operations on the JSON data
df.createOrReplaceTempView("json_data")
val resultDF = spark.sql("SELECT * FROM json_data WHERE tenant_id ='123505328935'")

resultDF.show()


//Performing ETL : Window function
//row_number()
val windowSpec  = Window.partitionBy("department_id").orderBy("salary")
//ranking: row_number() window function is used to give the sequential row number starting from 1 to the result of each //window partition.
 df_yb.withColumn("row_number",row_number.over(windowSpec)).show()

//rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.
df_yb.withColumn("rank",rank().over(windowSpec)).show()

 //writing back  to YCQL: Persisting a Dataset to Database using Save command, following examples are equivalent
 df.write
   .cassandraFormat("employees_json_copy", "test")
   .mode("overwrite")
   .save()
 
//To verify
val sqlDF = spark.sql("SELECT * FROM ybcatalog.test.employees_json_copy").show(false)

//Native support of Json:
val df = spark.sql("SELECT * FROM ybcatalog.test.employees_json WHERE get_json_object(phone, '$.phone') = 1200");
df.show

//Using JSONB Column Pruning
val query = "SELECT department_id, employee_id, get_json_object(phone, '$.code') as code FROM ybcatalog.test.employees_json WHERE get_json_string(phone, '$.key(1)') = '1400' order by department_id limit 2";
val df_sel1=spark.sql(query)
df_sel1.explain

//Predicate pushed down	   	   
val query = "SELECT department_id, employee_id, get_json_object(phone, '$.key[1].m[2].b') as key FROM ybcatalog.test.employees_json WHERE get_json_string(phone, '$.key[1].m[2].b') = '400' order by department_id limit 2";

val df_sel2 = spark.sql(query)
df_sel2.show()
df_sel2.explain
println("Json processing successful")
  }


https://www.projectpro.io/recipes/work-with-complex-nested-json-files-using-spark-sql
