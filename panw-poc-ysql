--POC cannon
ssh -i "kp-CANNON-aws.pem" centos@50.16.67.143

--Universe: alan-panw-audit-demo
sudo ssh -i /opt/yugabyte/yugaware/data/keys/3315aeea-aa87-43af-9c38-f72c20b9d8c0/yb-demo-aws-poc-selective_3315aeea-aa87-43af-9c38-f72c20b9d8c0-key.pem -ostricthostkeychecking=no -p 22 yugabyte@10.36.1.8

database: yugabyte
password: Panwaudit2023!
table: test

--app server
ssh -i "kp-CANNON-aws.pem" centos@44.214.44.45

--Create a keystore to access ssl enabled YCQL
scp -i "kp-CANNON-aws.pem" ~/Downloads/alan.crt centos@44.214.44.45:spark3yb/root.crt

--start Spark
cd /home/centos/spark-3.4.1-bin-hadoop3
./bin/spark-shell --packages com.yugabyte:jdbc-yugabytedb:42.3.0

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import java.util.Properties
import org.apache.spark.sql.SaveMode



val jdbcUrl = s"jdbc:yugabytedb://10.36.2.198:5433/yugabyte"
val host = "10.36.2.198"
//val keyspace = "panw_test"
val table = "test1"
val user = "yugabyte"
val password = "Panwaudit2023!"

val connectionProperties = new Properties()
    connectionProperties.put("user", s"yugabyte")
    connectionProperties.put("password", s"Panwaudit2023!")
    connectionProperties.setProperty("Driver", "com.yugabyte.Driver")

//
val test_Df = spark.read.jdbc(jdbcUrl, "test1", connectionProperties)
test_Df.show()
test_Df.printSchema()

//
val test_Df = spark.read.jdbc(jdbcUrl, table="(select * from test1) test_alias", connectionProperties)

//DataFrame API
test_Df.select("id","ceil").groupBy("ceil").sum("id").limit(10).show

//Spark.SQL API
test_Df.createOrReplaceTempView("test1")
spark.sql("select ceil, sum(id) from test1 group by ceil limit 10").show

//Save
val test_copy_Df = spark.read.jdbc(jdbcUrl, table="(select * from test1_copy) test_copy_alias", connectionProperties)
test_copy_Df.createOrReplaceTempView("test1_copy")

//write to new table: ww
spark.table("test1_copy").write.mode(SaveMode.Append).jdbc(jdbcUrl, "ww", connectionProperties)


//parallelism, verify from https://localhost:4040.
val new_test_df = spark.read.format("jdbc")
              .option("url", "jdbc:yugabytedb://10.36.2.198:5433/yugabyte")
              .option("dbtable", "test1")
              .option("user", user)
              .option("password", password)
              .option("driver", "com.yugabyte.Driver")
              .option("load-balance", "true")
              .option("numPartitions", 5)
              .option("partitionColumn", "ceil")
              .option("lowerBound", 0)
              .option("upperBound", 20)
              .option("pushDownPredicate", true)
              .option("pushDownAggregate", true).load

new_test_df.createOrReplaceTempView("test1")
spark.sql("select sum(ceil) from test where id > 50000").show


} }
