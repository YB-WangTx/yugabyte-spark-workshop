--POC cannon
ssh -i "kp-CANNON-aws.pem" centos@50.16.67.143

--Universe: alan-panw-audit-demo
sudo ssh -i /opt/yugabyte/yugaware/data/keys/3315aeea-aa87-43af-9c38-f72c20b9d8c0/yb-demo-aws-poc-selective_3315aeea-aa87-43af-9c38-f72c20b9d8c0-key.pem -ostricthostkeychecking=no -p 22 yugabyte@10.36.1.8

database: yugabyte
password: Panwaudit2023!
table: test

--app server
ssh -i "kp-CANNON-aws.pem" centos@44.214.44.45

--Create a keystore to access ssl enabled YCQL
scp -i "kp-CANNON-aws.pem" ~/Downloads/alan.crt centos@44.214.44.45:spark3yb/root.crt

--start Spark
cd /home/centos/spark-3.4.1-bin-hadoop3
./bin/spark-shell --packages com.yugabyte:jdbc-yugabytedb:42.3.0

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import java.util.Properties
import org.apache.spark.sql.SaveMode


val jdbcUrl = s"jdbc:yugabytedb://localhost:5433/ysql_spark_shell"

val connectionProperties = new Properties()
    connectionProperties.put("user", s"yugabyte")
    connectionProperties.put("password", s"Panwaudit2023!")
    connectionProperties.setProperty("Driver", "com.yugabyte.Driver")

//
val test_Df = spark.read.jdbc(jdbcUrl, "test", connectionProperties)
test_Df.show()
test_Df.printSchema()

//
val test_Df = spark.read.jdbc(jdbcUrl, table="(select * from test) test_alias", connectionProperties)

//DataFrame API
test_Df.select("id","ceil").groupBy("ceil").sum("id").limit(10).show

//Spark.SQL API
test_Df.createOrReplaceTempView("test")
spark.sql("select ceil, sum(id) from test group by ceil limit 10").show

//Save
val test_copy_Df = spark.read.jdbc(jdbcUrl, table="(select * from test_copy) test_copy_alias"connectionProperties)
test_copy_Df.createOrReplaceTempView("test_copy")
spark.table("test_copy").write.mode(SaveMode.Append).jdbc(jdbcUrl, "test", connectionProperties)





} }
